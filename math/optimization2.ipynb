{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Something about optimization\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient function\n",
    "A gradient, also known as Jacobian function, is the 1st-order derivative or partial differentials of a function $f$. In mathematics, the gradient can be writen as the following forms:\n",
    "$$\n",
    "{\\bf g}({\\bf x}) = \\nabla_{\\bf x} f({\\bf x}) = \\frac{\\partial f({\\bf x})}{\\partial {\\bf x}} \\\\\n",
    "$$\n",
    "where ${\\bf g}({\\bf x})$ is a vector of partial differentials of $f({\\bf x})$ at ${\\bf x} = (x_1, x_2, \\ldots, x_n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian function\n",
    "\n",
    "The second-order differential of a function, is called a Hessian function, which is a matrix of $n \\times n$ (where $n$ is the length of the vector ${\\bf x}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex optimization\n",
    "\n",
    "All gradient methods share the same implicit assumption:\n",
    ">> The objective function has single global minimum. In other words, the likelihood function is \"convex\".\n",
    "\n",
    "For a given fuction, \n",
    ">> It is convex if and only if its Hessian (matrix of second derivatives) has negative eigenvalues everywhere.\n",
    "\n",
    "For linear models and Gaussian noise, this assumption is always satisfied, i.e., the likelihood is always convex. In this case, the Hessian is constant (indpendent of the fitting parameters) and all its eigenvalues are negative. This is clear because the Hessian is related to the covariance matrix via $\\Sigma = -H^{-1}$ and since $\\Sigma$ is positive definite (正定), so $H$ has to be negative definite (负定).\n",
    "\n",
    "Howeve, when it comes to nonlinear models and non-Gaussian noises, it is very difficult and even rarely possible to test for convexity.\n",
    "\n",
    "Moreover, nonlinear problems are often non-convex, such that gradient-based methods all run into the nearest local minimum, depending on the initial guess. Hence, gradient-based methods are highly efficient (fast) but not very robust. They often have the \"opposite\" behavior of the Simplex algorithm.\n",
    "\n",
    "Unlike `leastsq` and `curve_fit`, gradient-based methods can also be applied to fit problems that do not have a least-squares formulation. However, this requires us to be able to implement them on our own. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's method\n",
    "\n",
    "Newton's method is naturally a method for finding a root for an equation. Here we want to find the root of the first derivative of our objective funtion.\n",
    "\n",
    "The algorithm is iterative. Given a parameter vector ${\\bf \\theta}_i$, the next iteration is obtained by:\n",
    "$$\n",
    "{\\bf \\theta}_{i+1} = {\\bf \\theta}_i - H^{-1} \\cdot \\nabla_{\\bf \\theta} {\\bf J}({\\bf \\theta}_i)\n",
    "$$\n",
    "\n",
    "Obviously, we need to compute the gradient $\\nabla_{\\bf \\theta} {\\bf J}({\\bf \\theta})$ of our obejctive function ${\\bf J}({\\bf \\theta})$ and the Hessian (matrix of second derivatives). This usually requires a lot of math.\n",
    "\n",
    "In rare cases, the results are so simple that they can be easily implemented. If the assumption of a single global minimum holds (convex problem), then Newton's method will usually converge extremely fast (say, within 5-10 iterations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "One of the simplest optimization algorithms is called the **gradient descent** or **steepest descent**. This can be written as follows:\n",
    "$$\n",
    "{\\bf \\theta}_{k+1} = {\\bf \\theta}_k - \\eta_k {\\bf g}_k  = {\\bf \\theta}_k - \\eta_k \\nabla f({\\bf \\theta})\n",
    "$$\n",
    "where $k$ indexes steps of the algorithm, ${\\bf g}_k = {\\bf g}({\\bf \\theta}_k)$ is the gradient at step $k$, and $\\eta_k$ is called the **learning rate** or **step size**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
